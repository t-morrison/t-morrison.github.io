---
title: "Wells Fargo Complaints Text Analysis"
author: "T Morrison"
date: "September 15, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(httr)
library(ggplot2)
library(SnowballC)
library(RColorBrewer)
library(data.table)
library(ggthemes)
library(scales)
library(tm)
library(wordcloud)
#rmarkdown::render("C:/Users/tmo/Documents/GitHub/moman822.github.io/projects/CFPB/CFPB3.Rmd")
```

```{r}
cfpb <- GET("https://data.consumerfinance.gov/resource/jhzv-w97w.csv")
cfpb <- httr::content(cfpb)
setDT(cfpb)
cfpb$date_received <- as.Date(cfpb$date_received, format="%m%d%y")
Wells <- cfpb[company=="Wells Fargo & Company"]
```

In a previous [project](https://moman822.github.io/projects/CFPB/CFPB3.html) I discussed the recent revelations that Wells Fargo had been inappropriately pushing new products onto customers and in many cases opening accounts without the customers knowing it. There's been a big backlash in Washington directed towards the upper management since it came out about a month ago, and Wells Fargo was fined some $185 million.

I've read some things saying that the CFPB didn't *really* have that much to do with nailing Wells Fargo for this, but rather jumped on the bandwagon after some people in LA got the scoop. I want to see whether the CFPB's consumer complaint system could have given an early warning of this dirty dealing.

I am using the `tm` package in R to do some text mining/analysis on the consumer narratives that are included with some of the complaints. The data is available on the CFPB's website.

The consumer narratives are optional, and only 101,950 out of a total 638,524 complaints since December, 2011 have anything for that category.

To do the analysis, I have to set up a *corpus*, which is a collection of all the separate text bodies. I will do this only for complaints against Wells Fargo first.
```{r}
Wells <- Wells[!is.na(Wells$complaint_what_happened)]
WF <- Wells$complaint_what_happened
WF <- VCorpus(VectorSource(WF))
```

Then it is neccesary to some preprocessing to remove punctuation, remove common words that don't give any useful context (e.g. "the","at"), and make everything lower case.

```{r}
WF <- tm_map(WF, stripWhitespace)
WF <- tm_map(WF, content_transformer(tolower))
WF <- tm_map(WF, removeWords, stopwords('english'))
WF <- tm_map(WF, stemDocument)
WF <- tm_map(WF, removePunctuation)
WF <- tm_map(WF, removeWords, c("xxxx","xx/xx/xxxx","XXXX","XXXX,","xxxx,", "well","fargo", "xxxxxxxx"))
WF <- tm_map(WF, removeNumbers)
```

Using the `wordcloud` package, I can make a fun word cloud of the most common words in the complaints, sized by their frecuency.
```{r}
wordcloud(WF, scale=c(3,.5), max.words=90, color=brewer.pal(8, "Dark2"))
```

This doesn't really say much, and it is not going to be very revealing looking at only the Wells Fargo complaints by themeselves. To see whether Wells Fargo customers were saying more in their complaints about some specific issues than normal, we need to compare it to some other bank's complaints.

We will look at Bank of America (BoA), who received the mot complaints, and JP Morgan/Chase, who received the fourth most complaints. Wells Fargo was second. The third most complaints went to Equifax, which is not a bank.

```{r}
BOA <- VCorpus(VectorSource(cfpb[company=="Bank of America"][!is.na(complaint_what_happened)]$complaint_what_happened))
JPM <- VCorpus(VectorSource(cfpb[company=="JPMorgan Chase & Co."][!is.na(complaint_what_happened)]$complaint_what_happened))

BOA <- tm_map(BOA, stripWhitespace)
BOA <- tm_map(BOA, content_transformer(tolower))
BOA <- tm_map(BOA, removeWords, stopwords('english'))
BOA <- tm_map(BOA, stemDocument)
BOA <- tm_map(BOA, removePunctuation)
BOA <- tm_map(BOA, removeWords, c("the","xxxx","xx/xx/xxxx","XXXX","XXXX,","xxxx,", "well","fargo", "xxxxxxxx"))
BOA <- tm_map(BOA, removeNumbers)
JPM <- tm_map(JPM, stripWhitespace)
JPM <- tm_map(JPM, content_transformer(tolower))
JPM <- tm_map(JPM, removeWords, stopwords('english'))
JPM <- tm_map(JPM, stemDocument)
JPM <- tm_map(JPM, removePunctuation)
JPM <- tm_map(JPM, removeWords, c("xxxx","xx/xx/xxxx","XXXX","XXXX,","xxxx,", "well","fargo", "xxxxxxxx"))
JPM <- tm_map(JPM, removeNumbers)
```

JP Morgan has 3,877 complaint narratives, BoA has 5015, and Wells Fargo has 4617.

To do some serious analysis, I need to make a document-term matrix for each company's complaints. This is a matrix with every word on one axis and every document (individual complaint) on the other axis, and whether.
```{r}
dtmWF <- DocumentTermMatrix(WF)
dtmBOA <- DocumentTermMatrix(BOA)
dtmJPM <- DocumentTermMatrix(JPM)
```

Here are five rows and five columns for Wells Fargo. You will notice that these words did not appear in any of those five documents. There are over 13,000 unique words in this corpus for Wells Fargo, so a lot of the matrix is empty.
```{r}
inspect(dtmWF[1:5,1000:1005])
```

What were the most common words in each set of complaints?

Wells Fargo:
```{r}
freqWF <- colSums(as.matrix(dtmWF))
ordWF <- order(freqWF, decreasing=T)
freqWF[head(ordWF,10)]
```

BoA:
```{r}
freqBOA <- colSums(as.matrix(dtmBOA))
ordBOA <- order(freqBOA, decreasing=T)
freqBOA[head(ordBOA,10)]
```

JP Morgan/Chase
```{r}
freqJPM <- colSums(as.matrix(dtmJPM))
ordJPM <- order(freqJPM, decreasing=T)
freqJPM[head(ordJPM,10)]
```



```{r}
fraud <- DocumentTermMatrix(WF, list(dictionary=c("fraud", "unauthorized")))

findAssocs(dtm, "loan", .2)
```















